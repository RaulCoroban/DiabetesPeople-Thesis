\subsection{Data set}
The original data contains 488 patient samples across 78980 SNPs. Data has been obtained through agreements with \emph{Hospital Universitari i Politècnic La Fe} from Valencia. It's represented as a matrix of codes [0, 1, 2], being 0 = homozygous non-reference, 1 = heterozygous, 2 = homozygous reference.

\subsection{Pipelining \emph{ensemble} results}
\label{section:methods:pipeline}
A run on the \emph{ensemble} tool results in an object containing all the information from each sequential run, its inner splits and features used for prediction.

We reproduce the same conditions to obtain the Reference Observation (RO):

The new module loads an ensemble object, get its top variables (topN) and iterates over the contained sequentials. Each sequential has S data splits, which we must itearte over. For each set of selected variables from RO in a sequential, a model is re-trained and tested in order to obtain a swap result (SO). After, each variable from sequential top is swapped by another coming from ensemble topN (if it’s not in the set yet) to run a new model training (using the same data split) and aim for performance contrast.
\\

\emph{Pseudocode}
Loop for each S in sequential:
\begin{itemize}
    \item Loop for each (Train, Test) in S:
    \begin{itemize}
        \item Train model on Train using Original Vars
        \item Evaluate on Test: generate Reference Observation (RO)
        \item Loop for each U in Original Vars:
        \begin{itemize}
            \item Loop for each V in topN not in Original Vars:
            \begin{itemize}
                \item Swap U for V
                \item Re-train model on Train split.
                \item Re-evaluate on Test split: generate Swap Observation (S))
                \item Compare RO and SO scores and predictions.
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{itemize}



Swapping example:\\
\begin{center}
topN = [A, B, D, E]\\
Original Vars (OVars) = [B, C, D, F, G]\\
swap($\forall$ V $\in$ topN $\and \not \in$ OVars)\\    
\end{center}

\begin{table}[!htbp]
\centering
\resizebox{0.45\textwidth}{!}{%
\begin{tabular}{l|c|c|c|c|c|}
\cline{2-6}
                                                        & \multicolumn{5}{c|}{Feature set}                                                                                                                                                         \\ \hline
\multicolumn{1}{|l|}{Original}                          & B                                  & C                                  & D                                  & G                                  & F                                  \\ \hline
\multicolumn{1}{|l|}{}                                  & \cellcolor[HTML]{FFCC67}\textbf{A} & \cellcolor[HTML]{F1F8F6}C          & \cellcolor[HTML]{F1F8F6}D          & \cellcolor[HTML]{F1F8F6}G          & \cellcolor[HTML]{F1F8F6}F          \\ \cline{2-6} 
\multicolumn{1}{|l|}{}                                  & B                                  & \cellcolor[HTML]{FFCC67}\textbf{A} & D                                  & G                                  & F                                  \\ \cline{2-6} 
\multicolumn{1}{|l|}{}                                  & \cellcolor[HTML]{F1F8F6}B          & \cellcolor[HTML]{F1F8F6}C          & \cellcolor[HTML]{FFCC67}\textbf{A} & \cellcolor[HTML]{F1F8F6}G          & \cellcolor[HTML]{F1F8F6}F          \\ \cline{2-6} 
\multicolumn{1}{|l|}{}                                  & B                                  & C                                  & D                                  & \cellcolor[HTML]{FFCB2F}\textbf{A} & F                                  \\ \cline{2-6} 
\multicolumn{1}{|l|}{\multirow{-5}{*}{Swap Iterations}} & \cellcolor[HTML]{F1F8F6}B          & \cellcolor[HTML]{F1F8F6}C          & \cellcolor[HTML]{F1F8F6}D          & \cellcolor[HTML]{F1F8F6}G          & \cellcolor[HTML]{FFCC67}\textbf{A} \\ \hline
\end{tabular}%
}
\quad
\resizebox{0.45\textwidth}{!}{%
\begin{tabular}{l|c|c|c|c|c|}
\cline{2-6}
                                                        & \multicolumn{5}{c|}{Feature set}                                                                                                                                                         \\ \hline
\multicolumn{1}{|l|}{Original}                          & B                                  & C                                  & D                                  & G                                  & F                                  \\ \hline
\multicolumn{1}{|l|}{}                                  & \cellcolor[HTML]{68CBD0}\textbf{E} & \cellcolor[HTML]{F1F8F6}C          & \cellcolor[HTML]{F1F8F6}D          & \cellcolor[HTML]{F1F8F6}G          & \cellcolor[HTML]{F1F8F6}F          \\ \cline{2-6} 
\multicolumn{1}{|l|}{}                                  & B                                  & \cellcolor[HTML]{68CBD0}\textbf{E} & D                                  & G                                  & F                                  \\ \cline{2-6} 
\multicolumn{1}{|l|}{}                                  & \cellcolor[HTML]{F1F8F6}B          & \cellcolor[HTML]{F1F8F6}C          & \cellcolor[HTML]{68CBD0}\textbf{E} & \cellcolor[HTML]{F1F8F6}G          & \cellcolor[HTML]{F1F8F6}F          \\ \cline{2-6} 
\multicolumn{1}{|l|}{}                                  & B                                  & C                                  & D                                  & \cellcolor[HTML]{68CBD0}\textbf{E} & F                                  \\ \cline{2-6} 
\multicolumn{1}{|l|}{\multirow{-5}{*}{Swap Iterations}} & \cellcolor[HTML]{F1F8F6}B          & \cellcolor[HTML]{F1F8F6}C          & \cellcolor[HTML]{F1F8F6}D          & \cellcolor[HTML]{F1F8F6}G          & \cellcolor[HTML]{68CBD0}\textbf{E} \\ \hline
\end{tabular}%
}
\label{tbl:swap-example}
\caption{Generated sets for re-training using [A, B, D, E] as topN and [B, C, D, F, G] as OVars. Rows under "Swap Iterations" are the new features used for retraining, while "Original" will deliver the reference original prediction.}
\end{table}

\subsubsection{Metrics}
Depending on the kind of target we expect, we must adapt the metric used to compare the predicted targets. As we deal with a classification problem, we compare predicted labels using the accuracy score and assign a \textit{weak relavance} score from 0 to 1 to each pair of variables.

\subsection{Results extraction}
After a full run where all the possible swaps have been made, a list of pairs of variables and their scores is returned. Only results scoring above 0.75/1 accuracy and 0.95/1 weak relevance are saved by default. The results are then clustered in a relation matrix of \textit{removed variables} across \textit{added variables} as shown in Table \ref{tbl:swapvars}. 
\\

Each time a swap result has passed the threshold, we increment the strenght of the link among the implied variables by 1. As a result, we obtain a graph network whose edges represent the weight of the weak relevance score.

\subsubsection{Correction}
The \emph{ensemble} method delivers the most voted features as the result set (RS). The items $\in$ RS are swapped for each element of the original set (OS), therefore, due to the nature of the problem, there is a higher chance for an item $\in$ RS to be rejected, as its number os votes is higher, i.e. the more votes it gets, the more present will be in OS, and therefore, the swap will not be performed. The procedure actively avoids having duplicated features in the train/test to prevent multicollinearity problems. Due to this, as a post-processing step, we correct the number of times a swap has been successfully performed by adding a weighted score instead of 1:

\begin{center}
    Weak Relavance Score = \#successful swaps / \#all attempts to swap overall
\end{center}

Using the example detailed in \ref{tbl:swap-example}, variables A and E are the only ones allowed to be swapped. B and D cannot be used for this particular Original Vars set. For this iteration, the number of attempts to swap is 0 for B and D, however, in another set they might have a chance.
\\

On the other hand, each sequential has N splits of data. Since the split loop contains the swap ones, the same swap can be performed N times, yielding different results each time. We’ve decided to cluster the results per swap applying a mean over all the split results.

\subsubsection{Time Complexity}
The method contains 4 inner loops, spread across 4 protected methods.
\\

\textbf{Best case scenario:} ($\forall$ v $in$ V, v $\in$ T, where U = v $\in$ V, t $\in$ T, v == t) then:
\begin{center}
Min. Iterations $\approx$ S $\times$ P $\times$ V $\times$ T - U\\    
\end{center}

\textbf{Worst case scenario:} ($\forall$ v $in$ V, v $\in$ T, where U = v $\in$ V, t $\in$ T, v != t) then:
\begin{center}
Max. Iterations $\approx$ S $\times$ P $\times$ V $\times$ T\\    
\end{center}


Where: \textit{Sequentials amount} = S, \textit{Splits per sequential} = P, \textit{Variable set per split} = V and \textit{number of ensemble’s top variables} = T

\begin{table}[]
\centering
\resizebox{0.5\textwidth}{!}{%
\begin{tabular}{cr|c|c|c|c|c|c|}
\cline{3-8}
                                                       & \multicolumn{1}{c|}{} & \multicolumn{6}{c|}{Added}                                                      \\ \cline{3-8} 
\multicolumn{1}{l}{}                                   &                       & \textbf{F01} & \textbf{F02} & \textbf{F03} & \textbf{F04} & \textbf{F05} & \textbf{F06} \\ \hline
\multicolumn{1}{|c|}{\multirow{6}{*}{\rotatebox{90}{Removed}}} & \textbf{F01}          & -            & 2            & 4            & 3            & 2            & 0            \\ \cline{2-8} 
\multicolumn{1}{|c|}{}                                 & \textbf{F02}          & 2            & -            & 3            & 0            & 2            & 4            \\ \cline{2-8} 
\multicolumn{1}{|c|}{}                                 & \textbf{F03}          & 3            & 4            & -            & 1            & 2            & 3            \\ \cline{2-8} 
\multicolumn{1}{|c|}{}                                 & \textbf{F04}          & 4            & 0            & 2            & -            & 0            & 5            \\ \cline{2-8} 
\multicolumn{1}{|c|}{}                                 & \textbf{F05}          & 5            & 1            & 5            & 1            & -            & 2            \\ \cline{2-8} 
\multicolumn{1}{|c|}{}                                 & \textbf{F06}          & 0            & 3            & 3            & 4            & 2            & -            \\ \hline
\end{tabular}%
}
\label{tbl:swapvars}
\caption{Weak relevance relation matrix. Row indexes include the removed variable from the original set and column names contain the variables they have been swapped for. The relation value summarises the number of times a swap has resulted in a score > 0.75 and the weak relevance score > 0.95.}
\end{table}

\subsection{Grouping variables}

The relation plots such as Chords are built on relation tables (Table \ref{tbl:swapvars}) among variables. Since the main aim of the procedure is to cluster variables based on their redundancy, all connected components must belong to the same group.
Graph representations improve displaying connected components which go through further analysis. With a reduced number of variables as the toy example, the grouping does not need coherence checking.

Figure ?? shows the comparison and clustering. When the graph becomes too tangled for static plots, interactive versions are available.
In order to cluster the connected components we can follow two strategies:

\begin{enumerate}
    \item Connected component grouping: Cluster all the nodes that hold at least a connection.
    \begin{itemize}
        \item Requires a T threshold for the \emph{weak relevance} score limit.
        \item Filter by \emph{weak relevance} score (number of links per total number of swaps)
        \item Optionally filter by score gain.
    \end{itemize}
    
    \item Link strength grouping: Creates M groups, based on threshold ranges.
\end{enumerate}


\subsubsection{Approaches}
\label{methods:grouping:approaches}
We have developed different mechanisms of establishing relations among variables. They all group variables and set a \textit{representative} of the group. The elect item will replace all occurrences of any contained item.
\\

\textbf{Weak relevance}\\
Each connected component from the previous steps picks a representative by maximum connectivity. If there is only one connected component, this will be checked for inner community structures using the \_ algorithm, which will split the graph into >1 connected components.

As we deal with SNP information, we check the information among two SNPs, i.e. overlap their vectors and extract a Jaccard Similarity Score (JSC). Most of the data include either a majority of 0's or a majority of 1's, therefore, Jaccard scores near 0 and 1 are expected. We set the Jaccard information to filter scores below 0.2 and above 0.8 to correct the graph construction prior to grouping and aim for the highly similar features.
\\

\textbf{Spearman Correlation}\\
All features have been compared to each other to assess their Spearman correlation score. Each pair obtaining an absolute score above 0.95 are \emph{linked} together under the same group. The representative is picked randomly.

For the following grouping approaches, we use the labels of each SNP, which follow the format: (chromosome-locus-.-referenceAllele-observedAllele).
\\

\textbf{1k loci distance}\\
We group SNPs which are at a maximum of 1000 bases distance using Python scripts which split labels and merge Data Frame labels (column names).
\\

\textbf{10k loci distance}\\
We group SNPs which are at a maximum of 10.000 bases distance using Python scripts which split labels and merge Data Frame labels (column names).
\\

\textbf{Gene translation}
Each label has been trimmed to extract chromosome number and locus. This information is used to query the RS identifier of an SNP, and retrieve its Gene Symbol and consequence. SNPs belonging to the same gene are grouped. If the SNPs is targeted as a non-synonymous mutation we assume the product will be altered, hence the grouping is more accurate.

\subsection{Application to original results}
We use an already calculated \emph{ensemble} to perform the swaps. Within this model, we find information about the features picked individially by \emph{sequential}. Counting the votes of each sequential delivers the final vote score per feature.
\\

Each of the approaches in subsection \ref{methods:grouping:approaches} establish a relation among features that might be competeing for the votes. Grouping allows us to aggregate their votes and overcome vote split.